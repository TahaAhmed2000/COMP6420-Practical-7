{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvcf0YTruIvU"
      },
      "source": [
        "## These exercises will help you with the skills you will need in the assignments\n",
        "\n",
        "# 0. Text encoding\n",
        "\n",
        "There is a file called \"story-cp1252.txt\" you will use in this practical. It has the special open-quote and close-quote characters from code page 1252.\n",
        "\n",
        "Open it up in two different programs that will render it differently, and take a screenshot of each.\n",
        "\n",
        "Be creative! Suggestions include Windows Notepad and the more command in a CMD prompt window. On OSX, Microsoft Word (if you specify Windows Latin 1 as the encoding) can render the quotes; most other programs won't.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE6f_D8iuIvZ"
      },
      "source": [
        "*This cell is where you can put the screenshots*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODJMtOrnuIva"
      },
      "source": [
        "### Crasher\n",
        "\n",
        "The following cell will fail with a unicode exception. Fix it."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2asIULSn202Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "IHnXwe9puIvb",
        "outputId": "63a07741-e2b0-412f-b1c7-23fa7643fe37"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0x93 in position 376: invalid start byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7d7bfc0b285c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Practical 7/story-cp1252.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x93 in position 376: invalid start byte"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/Practical 7/story-cp1252.txt') as f:\n",
        "    f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgFBfYQTuIvc"
      },
      "source": [
        "### Copying\n",
        "\n",
        "Make a copy of story-cp1252.txt in **utf-8 format** and then look at it using some other tool on your computer. (e.g.\n",
        "Windows notepad)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TulLXMijuIvc"
      },
      "outputs": [],
      "source": [
        "# this is where to put your code\n",
        "# this is where to put your code\n",
        "with open('/content/drive/MyDrive/Practical 7/story-cp1252.txt',encoding='cp1252') as f:\n",
        "   with open('/content/drive/MyDrive/Practical 7/story-utf8.txt','w',encoding='utf-8') as g:\n",
        "       g.write(f.read())\n",
        "   # f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xepIXQCquIvd"
      },
      "source": [
        "Make a copy of story-cp1252.txt in **utf-16 format**. Check the size of it in bytes. If you are using Linux or OSX, the UTF-16 file may look like it is corrupted. On Windows it will open normally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JVRQnT6muIvd"
      },
      "outputs": [],
      "source": [
        "# this is where to put your code# this is where to put your code\n",
        "with open('/content/drive/MyDrive/Practical 7/story-cp1252.txt',encoding='cp1252') as f:\n",
        "   with open('/content/drive/MyDrive/Practical 7/story-utf16.txt','w',encoding='utf-16') as g:\n",
        "       g.write(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YLocKRQuIve"
      },
      "source": [
        "# 1. Simple Statistics and NLTK\n",
        "\n",
        "The following exercises use a portion of the Gutenberg corpus that is stored in the corpus dataset of NLTK. [The Project Gutenberg](http://www.gutenberg.org/) is a large collection of electronic books that are out of copyright. These books are free to download for reading, or for our case, for doing a little of corpus analysis.\n",
        "\n",
        "To obtain the list of files of NLTK's Gutenberg corpus, type the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0xrg8zNuIve",
        "outputId": "5793a0bc-6e67-497f-91fd-5d438e783ea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.corpus.gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMlmBUxpuIvf"
      },
      "source": [
        "To obtain all words in the entire Gutenberg corpus of NLTK, type the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MryGwtttuIvf"
      },
      "outputs": [],
      "source": [
        "gutenbergwords = nltk.corpus.gutenberg.words()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n10IRUY8uIvf"
      },
      "source": [
        "Now you can find the total number of words, and the first 10 words (do not attempt to display all the words or your computer will freeze!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xEvZdmFuIvg",
        "outputId": "321c944e-822a-4234-e2df-b02b80647b94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2621613"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "len(gutenbergwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiXtDlPVuIvg",
        "outputId": "4103ce5d-86dc-4c59-e721-bf67186845d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "gutenbergwords[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnJEyVaAuIvg"
      },
      "source": [
        "You can also find the words of just a selection of documents, as shown below. For more details of what information you can extract from this corpus, read the \"Gutenberg corpus\" section of the [NLTK book chapter 2](http://www.nltk.org/book_1ed/ch02.html), section 2.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH6X8JYduIvg",
        "outputId": "02f1fe35-8295-486c-aee7-8498b3a8ecb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192427"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "len(emma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odO0f6iIuIvg",
        "outputId": "47a0df4f-97b9-49a8-f61c-5b0cf8e98e46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "emma[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciR7d3B4uIvg"
      },
      "source": [
        "As we have seen in the lectures, we can use Python's `collections.Counter` to find the most frequent words of a document from NLTK's Gutenberg collection. Below you can see how you can find the 5 most frequent words of the word list stored in the variable `emma`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1QaxESwuIvg",
        "outputId": "84d38a70-60af-4cd5-805a-af67b91b162e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 11454), ('.', 6928), ('to', 5183), ('the', 4844), ('and', 4672), ('of', 4279), ('I', 3178), ('a', 3004), ('was', 2385), ('her', 2381), (';', 2199), ('it', 2128), ('in', 2118), ('not', 2101), ('\"', 2004), ('be', 1970), ('she', 1778), ('that', 1730), ('you', 1677), ('had', 1606)]\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "emma_counter = collections.Counter(emma)\n",
        "print(emma_counter.most_common(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaHUx1rtuIvh"
      },
      "source": [
        "### Exercise 1.1\n",
        "*Write Python code that prints the 10 most frequent words in each of the documents of the Gutenberg corpus. Can you identify any similarities among these list of most frequent words?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVdR8NKjuIvh",
        "outputId": "72f3756d-b134-401a-d9cc-d41bc79b3c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 186091), ('the', 125748), ('and', 78846), ('.', 73746), ('of', 70078), (':', 47406), ('to', 46443), ('a', 32504), ('in', 31959), ('I', 30221)]\n"
          ]
        }
      ],
      "source": [
        "# put your code here\n",
        "import collections\n",
        "gutenburg_counter = collections.Counter(gutenbergwords)\n",
        "print(gutenburg_counter.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for fileID in nltk.corpus.gutenberg.fileids():\n",
        "  words = nltk.corpus.gutenberg.words(fileID)\n",
        "  counter = collections.Counter(words)\n",
        "  print(fileID)\n",
        "  print(counter.most_common(10))\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjznn0qs4ZOy",
        "outputId": "67e7ee6d-ee3a-4f37-c92d-6b66b6e607fe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "austen-emma.txt\n",
            "[(',', 11454), ('.', 6928), ('to', 5183), ('the', 4844), ('and', 4672), ('of', 4279), ('I', 3178), ('a', 3004), ('was', 2385), ('her', 2381)]\n",
            "\n",
            "austen-persuasion.txt\n",
            "[(',', 6750), ('the', 3120), ('to', 2775), ('.', 2741), ('and', 2739), ('of', 2564), ('a', 1529), ('in', 1346), ('was', 1330), (';', 1290)]\n",
            "\n",
            "austen-sense.txt\n",
            "[(',', 9397), ('to', 4063), ('.', 3975), ('the', 3861), ('of', 3565), ('and', 3350), ('her', 2436), ('a', 2043), ('I', 2004), ('in', 1904)]\n",
            "\n",
            "bible-kjv.txt\n",
            "[(',', 70509), ('the', 62103), (':', 43766), ('and', 38847), ('of', 34480), ('.', 26160), ('to', 13396), ('And', 12846), ('that', 12576), ('in', 12331)]\n",
            "\n",
            "blake-poems.txt\n",
            "[(',', 680), ('the', 351), ('.', 201), ('And', 176), ('and', 169), ('of', 131), ('I', 130), ('in', 116), ('a', 108), (\"'\", 104)]\n",
            "\n",
            "bryant-stories.txt\n",
            "[(',', 3481), ('the', 3086), ('and', 1873), ('.', 1817), ('to', 1165), ('a', 988), ('\"', 900), ('he', 872), ('of', 801), ('was', 706)]\n",
            "\n",
            "burgess-busterbrown.txt\n",
            "[('.', 823), (',', 822), ('the', 639), ('he', 562), ('and', 484), ('to', 426), (\"'\", 401), ('of', 326), ('that', 285), ('a', 275)]\n",
            "\n",
            "carroll-alice.txt\n",
            "[(',', 1993), (\"'\", 1731), ('the', 1527), ('and', 802), ('.', 764), ('to', 725), ('a', 615), ('I', 543), ('it', 527), ('she', 509)]\n",
            "\n",
            "chesterton-ball.txt\n",
            "[(',', 4547), ('the', 4523), ('.', 3589), ('of', 2529), ('and', 2488), ('a', 2184), ('\"', 1751), ('to', 1558), ('in', 1355), ('that', 1120)]\n",
            "\n",
            "chesterton-brown.txt\n",
            "[('the', 4321), (',', 4069), ('.', 2784), ('of', 2087), ('and', 2074), ('a', 2074), ('\"', 1461), ('to', 1378), ('in', 1205), ('was', 1141)]\n",
            "\n",
            "chesterton-thursday.txt\n",
            "[(',', 3488), ('the', 3291), ('.', 2717), ('a', 1713), ('of', 1710), ('and', 1568), ('\"', 1336), ('to', 1045), ('in', 888), ('I', 885)]\n",
            "\n",
            "edgeworth-parents.txt\n",
            "[(',', 15219), ('the', 7149), ('.', 6945), ('to', 5150), ('and', 4769), ('\"', 3880), ('of', 3730), ('I', 3656), (\"'\", 3293), ('a', 3017)]\n",
            "\n",
            "melville-moby_dick.txt\n",
            "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)]\n",
            "\n",
            "milton-paradise.txt\n",
            "[(',', 10198), ('and', 2799), ('the', 2505), (';', 2317), ('to', 1758), ('of', 1486), ('.', 1254), ('in', 1083), ('his', 986), ('with', 876)]\n",
            "\n",
            "shakespeare-caesar.txt\n",
            "[(',', 2204), ('.', 1296), ('I', 531), ('the', 502), (':', 499), ('and', 409), (\"'\", 384), ('to', 370), ('you', 342), ('of', 336)]\n",
            "\n",
            "shakespeare-hamlet.txt\n",
            "[(',', 2892), ('.', 1886), ('the', 860), (\"'\", 729), ('and', 606), ('of', 576), ('to', 576), (':', 565), ('I', 553), ('you', 479)]\n",
            "\n",
            "shakespeare-macbeth.txt\n",
            "[(',', 1962), ('.', 1235), (\"'\", 637), ('the', 531), (':', 477), ('and', 376), ('I', 333), ('of', 315), ('to', 311), ('?', 241)]\n",
            "\n",
            "whitman-leaves.txt\n",
            "[(',', 17713), ('the', 8814), ('and', 4797), ('of', 4127), ('I', 2932), (\"'\", 2362), ('to', 1930), ('-', 1774), ('.', 1769), ('in', 1714)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOxot0pbuIvh"
      },
      "source": [
        "### Exercise 1.2\n",
        "*Find the unique words with length of more than 17 characters in the complete Gutenberg corpus.*\n",
        "\n",
        "*Hint: to find the distinct items of a Python list you can convert it into a set:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4yq9cLtuIvh",
        "outputId": "cc406b79-b6e2-4279-d039-aa498cfe47b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a', 'b', 'c'}\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "my_list = ['a','b','c','a','c']\n",
        "my_set = set(my_list)\n",
        "print(my_set)\n",
        "print(len(my_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Xm3aA8yuIvh",
        "outputId": "9b627627-63c5-406e-8448-876ae6fff5c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Mahershalalhashbaz', 'characteristically', 'uninterpenetratingly'}\n"
          ]
        }
      ],
      "source": [
        "# put your code here\n",
        "# unique_words = set(gutenbergwords)\n",
        "# for word in unique_words:\n",
        "#   if len(word) > 17:\n",
        "#     print(word)\n",
        "\n",
        "unique_words=[words for words in gutenbergwords if len(words)>17]\n",
        "print(set(unique_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-48T17EuIvh"
      },
      "source": [
        "### Exercise 1.3\n",
        "*Find the words that are longer than 5 characters and occur more than 2000 times in the complete Gutenberg corpus.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJY1WkviuIvh",
        "outputId": "70d0414e-c67b-4c13-da5d-a65a4a7d370d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'little', 'Israel', 'people', 'children', 'should', 'before', 'against'}\n"
          ]
        }
      ],
      "source": [
        "# put your code here\n",
        "# gutenburg_counter = collections.Counter(gutenbergwords)\n",
        "# for word in gutenburg_counter:\n",
        "#   if len(word) > 5 and gutenburg_counter[word] > 2000:\n",
        "#     print(word)\n",
        "\n",
        "word=[words for words in collections.Counter(gutenbergwords) if len(words) > 5 and gutenburg_counter[words] > 2000]\n",
        "print(set(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EloI6jOuIvi"
      },
      "source": [
        "### Exercise 1.4\n",
        "*Find the average number of words in the documents of the NLTK Gutenberg corpus.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPVJ_OZguIvi",
        "outputId": "e4c0a759-7846-4450-b43e-27e0c611b4bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "145645.16666666666\n"
          ]
        }
      ],
      "source": [
        "# put your code here\n",
        "gutenburgwords = nltk.corpus.gutenberg.words()\n",
        "# sum=0\n",
        "# for i in gutenbergwords:\n",
        "#   sum+=1\n",
        "# print(sum/len(gutenbergwords))\n",
        "# print(gutenburg_counter/len(gutenbergwords))\n",
        "\n",
        "print(len(gutenburgwords)/len(nltk.corpus.gutenberg.fileids()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxa7saROuIvi"
      },
      "source": [
        "### (Optional) Exercise 1.5\n",
        "*Find the Gutenberg document that has the longest average word length.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCWDtCcauIvi",
        "outputId": "44940406-3c8a-433a-b1dd-79c856994953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file idausten-emma.txt, length of words192427 and num of char887071, and avg of words 4.609909212324673\n",
            "file idausten-persuasion.txt, length of words98171 and num of char466292, and avg of words 4.749793727271801\n",
            "file idausten-sense.txt, length of words141576 and num of char673022, and avg of words 4.753785952421314\n",
            "file idbible-kjv.txt, length of words1010654 and num of char4332554, and avg of words 4.286881563819072\n",
            "file idblake-poems.txt, length of words8354 and num of char38153, and avg of words 4.567033756284415\n",
            "file idbryant-stories.txt, length of words55563 and num of char249439, and avg of words 4.489300433741879\n",
            "file idburgess-busterbrown.txt, length of words18963 and num of char84663, and avg of words 4.464641670621737\n",
            "file idcarroll-alice.txt, length of words34110 and num of char144395, and avg of words 4.233216065669891\n",
            "file idchesterton-ball.txt, length of words96996 and num of char457450, and avg of words 4.716173862839705\n",
            "file idchesterton-brown.txt, length of words86063 and num of char406629, and avg of words 4.724783007796614\n",
            "file idchesterton-thursday.txt, length of words69213 and num of char320525, and avg of words 4.63099417739442\n",
            "file idedgeworth-parents.txt, length of words210663 and num of char935158, and avg of words 4.4391184023772565\n",
            "file idmelville-moby_dick.txt, length of words260819 and num of char1242990, and avg of words 4.76571875515204\n",
            "file idmilton-paradise.txt, length of words96825 and num of char468220, and avg of words 4.835734572682675\n",
            "file idshakespeare-caesar.txt, length of words25833 and num of char112310, and avg of words 4.347539968257655\n",
            "file idshakespeare-hamlet.txt, length of words37360 and num of char162881, and avg of words 4.3597698072805136\n",
            "file idshakespeare-macbeth.txt, length of words23140 and num of char100351, and avg of words 4.336689714779602\n",
            "file idwhitman-leaves.txt, length of words154883 and num of char711215, and avg of words 4.591950052620365\n",
            "4.835734572682675\n"
          ]
        }
      ],
      "source": [
        "# put your code here\n",
        "max_avg_word_length=0\n",
        "for fileID in nltk.corpus.gutenberg.fileids():\n",
        "  words = nltk.corpus.gutenberg.words(fileID)\n",
        "  counter = collections.Counter(words)\n",
        "  num_chars=len(nltk.corpus.gutenberg.raw(fileID))\n",
        "  avg_word_length=num_chars/len(words)\n",
        "  print(f'file id{fileID}, length of words{len(words)} and num of char{num_chars}, and avg of words {avg_word_length}')\n",
        "  if avg_word_length > max_avg_word_length:\n",
        "    max_avg_word_length = avg_word_length\n",
        "print(max_avg_word_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHPTLcuGuIvi"
      },
      "source": [
        "### Exercise 1.6\n",
        "*Find the 10 most frequent bigrams in the entire Gutenberg corpus.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAG4UuSIuIvi",
        "outputId": "2199f8ef-bf06-484b-ad7d-d010f01d2ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[((',', 'and'), 41294), (('of', 'the'), 18912), (('in', 'the'), 9793), ((\"'\", 's'), 9781), ((';', 'and'), 7559), (('and', 'the'), 6432), (('the', 'LORD'), 5964), ((',', 'the'), 5957), ((',', 'I'), 5677), ((',', 'that'), 5352)]\n"
          ]
        }
      ],
      "source": [
        "# put your code here\n",
        "words = nltk.corpus.gutenberg.words()\n",
        "bigrams = nltk.bigrams(words)\n",
        "bigrams_counter = collections.Counter(bigrams)\n",
        "print(bigrams_counter.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKBGbK7IuIvi"
      },
      "source": [
        "### Exercise 1.7\n",
        "*Find the most frequent bigram that begins with \"Moby\" in Herman Melville's \"Moby Dick\".*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJFe9Iw6uIvj",
        "outputId": "37eefe43-067a-4b86-8fc7-0dce64cb0da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({('Moby', 'Dick'): 83, ('Moby', '-'): 1})\n"
          ]
        }
      ],
      "source": [
        "# put your code here\n",
        "moby_words = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
        "moby_bigrams = nltk.bigrams(moby_words)\n",
        "# moby_bigrams_counter = collections.Counter(moby_bigrams)\n",
        "bigram_counts=collections.Counter((w1,w2) for (w1,w2) in moby_bigrams if w1=='Moby')\n",
        "print(bigram_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2RMsS-MuIvj"
      },
      "source": [
        "# 2. Text Preprocessing with NLTK\n",
        "The following exercises will ask questions about tokens, stems, and parts of speech.\n",
        "\n",
        "### Exercise 2.1\n",
        "*What is the sentence with the largest number of tokens in Austen's \"Emma\"?*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk # make sure nltk is installed\n",
        "import nltk\n",
        "\n",
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1yKoLo7E7Mc",
        "outputId": "a983a655-e311-4a54-d997-7d015a7c4654"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMW61jlnuIvj",
        "outputId": "5297fb5f-e2ad-4807-c69b-432f751bff5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "While he lived, it must be only an engagement;\n",
            "but she flattered herself, that if divested of the danger of\n",
            "drawing her away, it might become an increase of comfort to him.--\n",
            "How to do her best by Harriet, was of more difficult decision;--\n",
            "how to spare her from any unnecessary pain; how to make\n",
            "her any possible atonement; how to appear least her enemy?--\n",
            "On these subjects, her perplexity and distress were very great--\n",
            "and her mind had to pass again and again through every bitter\n",
            "reproach and sorrowful regret that had ever surrounded it.--\n",
            "She could only resolve at last, that she would still avoid a\n",
            "meeting with her, and communicate all that need be told by letter;\n",
            "that it would be inexpressibly desirable to have her removed just\n",
            "now for a time from Highbury, and--indulging in one scheme more--\n",
            "nearly resolve, that it might be practicable to get an invitation\n",
            "for her to Brunswick Square.--Isabella had been pleased with Harriet;\n",
            "and a few weeks spent in London must give her some amusement.--\n",
            "She did not think it in Harriet's nature to escape being benefited\n",
            "by novelty and variety, by the streets, the shops, and the children.--\n",
            "At any rate, it would be a proof of attention and kindness in herself,\n",
            "from whom every thing was due; a separation for the present; an averting\n",
            "of the evil day, when they must all be together again.\n",
            "The max num of word are: 275\n"
          ]
        }
      ],
      "source": [
        "# put your code here\n",
        "emma_text=nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
        "longest=''\n",
        "max_num_tokens=0\n",
        "for sentence in nltk.sent_tokenize(emma_text):\n",
        "  num_tokens=len(nltk.word_tokenize(sentence))\n",
        "  if num_tokens > max_num_tokens:\n",
        "    longest_sents=sentence\n",
        "    max_num_tokens=num_tokens\n",
        "print(longest_sents)\n",
        "print(f'The max num of word are: {max_num_tokens}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_EcrWxyuIvj"
      },
      "source": [
        "### Exercise 2.2\n",
        "*What are the 5 most frequent parts of speech in Austen's \"Emma\"? Use the universal tag set*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHn8GtYVuIvj",
        "outputId": "8842db96-bc46-44ef-b461-4ffee6e01b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "# put your code here\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUC1JyKAHl3N",
        "outputId": "59dcb9b2-e9b9-433c-d0a5-bca7ec187778"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emma_text=nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
        "emma_sents=[nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(emma_text)]\n",
        "emma_tagged_sents=nltk.pos_tag(emma_sents,tagset='universal')\n",
        "# print(emma_tagged_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "eAYrgqsiG5FJ",
        "outputId": "01aff4b3-e96d-41ed-f3a7-243b8066c22d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'isdigit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-4c1a3ab50134>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memma_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'austen-emma.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0memma_sents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memma_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0memma_tagged_sents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memma_sents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'universal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(emma_tagged_sents)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \"\"\"\n\u001b[1;32m    165\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             tag, conf = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             tag, conf = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"-\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"!HYPHEN\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"!YEAR\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'isdigit'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMubtyPKuIvt"
      },
      "source": [
        "### Exercise 2.3\n",
        "*What is the number of distinct stems in Austen's \"Emma\"?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1QUkgzMuIvt"
      },
      "outputs": [],
      "source": [
        "# put your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlfSNIx0uIvu"
      },
      "source": [
        "### (Optional) Exercise 2.4\n",
        "*What is the most ambiguous stem in Austen's \"Emma\"? (meaning, which stem in Austen's \"Emma\" is realised in the largest number of distinct tokens?)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijp59y3CuIvu"
      },
      "outputs": [],
      "source": [
        "# put your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH56xwyRuIvu"
      },
      "source": [
        "# 3. Post-practical questionnaire\n",
        "\n",
        "Please complete the week 7 post-practical questionnaire on iLearn."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "interpreter": {
      "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('comp3220': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}